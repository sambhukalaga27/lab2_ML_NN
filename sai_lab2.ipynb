{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Data Loading and Preprocessing**\n",
    "\n",
    "## **Introduction**\n",
    "The initial phase of this study involves **data loading and preprocessing**, which lays the foundation for effective analysis and machine learning modeling. The dataset under consideration contains **568,454 user-generated reviews**, capturing textual feedback along with metadata such as numerical ratings, timestamps, and helpfulness scores. Given the sheer volume and diversity of the dataset, meticulous preprocessing is critical to ensure reliability and accuracy in downstream tasks.\n",
    "\n",
    "## **Objectives of this Section**\n",
    "- **Efficiently load the dataset** while minimizing memory usage.\n",
    "- **Verify dataset integrity** by inspecting shape and column structure.\n",
    "- **Handle missing values and duplicates** to prevent distortions in analysis.\n",
    "- **Select only relevant features** that contribute to the classification task.\n",
    "- **Engineer a new feature (`Combined_Text`)** by merging short (`Summary`) and long (`Text`) reviews.\n",
    "- **Transform numerical ratings (`Score`) into categorical satisfaction levels (`User_Satisfaction`)** for supervised classification.\n",
    "\n",
    "## **Dataset Structure (Before Processing)**\n",
    "Upon loading the dataset, we retrieve the following properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (568454, 10)\n",
      "Columns: Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
      "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('Reviews.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "\n",
    "# Display dataset shape to confirm full loading\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "\n",
    "# Show column names to verify\n",
    "print(f\"Columns: {data.columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comprises **568,454 rows** and **10 columns**, capturing a mix of numerical, categorical, and textual data. \n",
    "\n",
    "## **Dataset Column Descriptions**\n",
    "\n",
    "The dataset consists of user-generated reviews, capturing essential details about products, users, ratings, and feedback. Each row represents an individual review, accompanied by metadata that provides context for the evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Column Descriptions**\n",
    "\n",
    "### **1. Id**\n",
    "- **Definition**: A unique identifier assigned to each row in the dataset.\n",
    "- **Type**: Integer\n",
    "\n",
    "### **2. ProductId**\n",
    "- **Definition**: A unique alphanumeric identifier associated with each product.\n",
    "- **Type**: String\n",
    "\n",
    "### **3. UserId**\n",
    "- **Definition**: A unique identifier assigned to each user who submitted a review.\n",
    "- **Type**: String\n",
    "\n",
    "### **4. ProfileName**\n",
    "- **Definition**: The display name of the user who submitted the review.\n",
    "- **Type**: String\n",
    "\n",
    "### **5. HelpfulnessNumerator**\n",
    "- **Definition**: The number of users who found the review helpful.\n",
    "- **Type**: Integer\n",
    "\n",
    "### **6. HelpfulnessDenominator**\n",
    "- **Definition**: The total number of users who provided feedback on whether a review was helpful.\n",
    "- **Type**: Integer\n",
    "\n",
    "### **7. Score**\n",
    "- **Definition**: The rating provided by the user, ranging from 1 to 5.\n",
    "- **Type**: Integer (1-5)\n",
    "- **Purpose**: Represents the user's sentiment about the product.\n",
    "  - **5** → Highly Satisfied\n",
    "  - **4** → Satisfied\n",
    "  - **3** → Neutral\n",
    "  - **2** → Not Satisfied\n",
    "  - **1** → Very Bad\n",
    "\n",
    "### **8. Time**\n",
    "- **Definition**: A Unix timestamp representing the date and time when the review was posted.\n",
    "- **Type**: Integer\n",
    "\n",
    "### **9. Summary**\n",
    "- **Definition**: A brief, high-level summary of the review provided by the user.\n",
    "- **Type**: String\n",
    "\n",
    "### **10. Text**\n",
    "- **Definition**: The full-length detailed review written by the user.\n",
    "- **Type**: String\n",
    "---\n",
    "\n",
    "## **Data Cleaning Steps**\n",
    "1. **Handling Missing Values**:\n",
    "   - Since the `Score` column is central to our classification task, all rows with `NaN` values in `Score` are removed.\n",
    "   - Missing values in textual fields (`Summary` and `Text`) are replaced with empty strings (`\"\"`) to ensure consistency in text processing.\n",
    "\n",
    "2. **Removing Duplicates**:\n",
    "   - Identical reviews can distort classification models by **artificially inflating the occurrence of certain labels**. Therefore, we eliminate duplicate rows.\n",
    "\n",
    "3. **Feature Selection**:\n",
    "   - To reduce computational complexity and enhance model interpretability, we retain only the most pertinent columns:\n",
    "     - `HelpfulnessNumerator`: Number of users who found the review helpful.\n",
    "     - `HelpfulnessDenominator`: Total number of users who rated helpfulness.\n",
    "     - `Score`: The numerical rating given by the reviewer (1 to 5).\n",
    "     - `Summary`: A brief overview of the review.\n",
    "     - `Text`: The full review content.\n",
    "\n",
    "4. **Feature Engineering (`Combined_Text`)**:\n",
    "   - The `Summary` column contains **concise expressions of user sentiment**, whereas the `Text` column provides **detailed qualitative feedback**.\n",
    "   - To **capture both short- and long-form sentiment**, we concatenate them into a new column:  \n",
    "     ```\n",
    "     Combined_Text = Summary + \" \" + Text\n",
    "     ```\n",
    "\n",
    "## **Transforming the `Score` Column**\n",
    "The numerical `Score` column is mapped to categorical `User_Satisfaction` levels as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Combined_Text</th>\n",
       "      <th>User_Satisfaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>Good Quality Dog Food I have bought several of...</td>\n",
       "      <td>Highly Satisfied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>Not as Advertised Product arrived labeled as J...</td>\n",
       "      <td>Very Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>\"Delight\" says it all This is a confection tha...</td>\n",
       "      <td>Satisfied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>Cough Medicine If you are looking for the secr...</td>\n",
       "      <td>Not Satisfied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>Great taffy Great taffy at a great price.  The...</td>\n",
       "      <td>Highly Satisfied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1299628800</td>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "      <td>Will not do without Great for sesame chicken.....</td>\n",
       "      <td>Highly Satisfied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1331251200</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "      <td>disappointed I'm disappointed with the flavor....</td>\n",
       "      <td>Not Satisfied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1329782400</td>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "      <td>Perfect for our maltipoo These stars are small...</td>\n",
       "      <td>Highly Satisfied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1331596800</td>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "      <td>Favorite Training and reward treat These are t...</td>\n",
       "      <td>Highly Satisfied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "      <td>Great Honey I am very satisfied ,product is as...</td>\n",
       "      <td>Highly Satisfied</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568454 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                          1                       1      5  1303862400   \n",
       "1                          0                       0      1  1346976000   \n",
       "2                          1                       1      4  1219017600   \n",
       "3                          3                       3      2  1307923200   \n",
       "4                          0                       0      5  1350777600   \n",
       "...                      ...                     ...    ...         ...   \n",
       "568449                     0                       0      5  1299628800   \n",
       "568450                     0                       0      2  1331251200   \n",
       "568451                     2                       2      5  1329782400   \n",
       "568452                     1                       1      5  1331596800   \n",
       "568453                     0                       0      5  1338422400   \n",
       "\n",
       "                                   Summary  \\\n",
       "0                    Good Quality Dog Food   \n",
       "1                        Not as Advertised   \n",
       "2                    \"Delight\" says it all   \n",
       "3                           Cough Medicine   \n",
       "4                              Great taffy   \n",
       "...                                    ...   \n",
       "568449                 Will not do without   \n",
       "568450                        disappointed   \n",
       "568451            Perfect for our maltipoo   \n",
       "568452  Favorite Training and reward treat   \n",
       "568453                         Great Honey   \n",
       "\n",
       "                                                     Text  \\\n",
       "0       I have bought several of the Vitality canned d...   \n",
       "1       Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2       This is a confection that has been around a fe...   \n",
       "3       If you are looking for the secret ingredient i...   \n",
       "4       Great taffy at a great price.  There was a wid...   \n",
       "...                                                   ...   \n",
       "568449  Great for sesame chicken..this is a good if no...   \n",
       "568450  I'm disappointed with the flavor. The chocolat...   \n",
       "568451  These stars are small, so you can give 10-15 o...   \n",
       "568452  These are the BEST treats for training and rew...   \n",
       "568453  I am very satisfied ,product is as advertised,...   \n",
       "\n",
       "                                            Combined_Text User_Satisfaction  \n",
       "0       Good Quality Dog Food I have bought several of...  Highly Satisfied  \n",
       "1       Not as Advertised Product arrived labeled as J...          Very Bad  \n",
       "2       \"Delight\" says it all This is a confection tha...         Satisfied  \n",
       "3       Cough Medicine If you are looking for the secr...     Not Satisfied  \n",
       "4       Great taffy Great taffy at a great price.  The...  Highly Satisfied  \n",
       "...                                                   ...               ...  \n",
       "568449  Will not do without Great for sesame chicken.....  Highly Satisfied  \n",
       "568450  disappointed I'm disappointed with the flavor....     Not Satisfied  \n",
       "568451  Perfect for our maltipoo These stars are small...  Highly Satisfied  \n",
       "568452  Favorite Training and reward treat These are t...  Highly Satisfied  \n",
       "568453  Great Honey I am very satisfied ,product is as...  Highly Satisfied  \n",
       "\n",
       "[568454 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows where 'Score' is empty\n",
    "data = data[data[\"Score\"].notna()]\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "\n",
    "# Select only the relevant columns\n",
    "selected_columns = [\n",
    "    \"HelpfulnessNumerator\", \n",
    "    \"HelpfulnessDenominator\", \n",
    "    \"Score\", \n",
    "    \"Time\", \n",
    "    \"Summary\", \n",
    "    \"Text\"\n",
    "]\n",
    "\n",
    "# Keep only the selected columns\n",
    "data = data[selected_columns]\n",
    "\n",
    "data[\"Score\"] = pd.to_numeric(data[\"Score\"], errors='coerce')\n",
    "data[\"Combined_Text\"] = data[\"Summary\"] + \" \" + data[\"Text\"]\n",
    "\n",
    "# Define function to categorize User_Satisfaction\n",
    "def classify_satisfaction(rating):\n",
    "    if rating == 5:\n",
    "        return \"Highly Satisfied\"\n",
    "    elif rating == 4:\n",
    "        return \"Satisfied\"\n",
    "    elif rating == 3:\n",
    "        return \"Neutral\"\n",
    "    elif rating == 2:\n",
    "        return \"Not Satisfied\"\n",
    "    elif rating <= 1:\n",
    "        return \"Very Bad\"\n",
    "    else:\n",
    "        return None  # Handle missing values gracefully\n",
    "\n",
    "# Apply function to create new column\n",
    "data[\"User_Satisfaction\"] = data[\"Score\"].apply(classify_satisfaction)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation enables the classification task to shift from a **numeric regression problem to a categorical classification problem**.\n",
    "\n",
    "## **Class Distribution (After Processing)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_Satisfaction\n",
      "Highly Satisfied    363122\n",
      "Satisfied            80655\n",
      "Very Bad             52268\n",
      "Neutral              42640\n",
      "Not Satisfied        29769\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "user_satisfaction_counts = data[\"User_Satisfaction\"].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(user_satisfaction_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Data Splitting for Machine Learning**\n",
    "\n",
    "## **Why Splitting the Data?**\n",
    "Before training a classification model, it is crucial to **partition the dataset into training and testing subsets**. This ensures:\n",
    "- **Generalizability**: The model is evaluated on unseen data.\n",
    "- **Avoiding Overfitting**: Prevents the model from memorizing patterns in training data.\n",
    "- **Measuring Model Performance**: Provides an unbiased estimate of accuracy.\n",
    "\n",
    "## **Data Preparation Steps**\n",
    "1. **Label Encoding**:\n",
    "   - Since `User_Satisfaction` is a categorical variable, it is transformed into **numerical labels** via `LabelEncoder()`.\n",
    "   - Example:\n",
    "     ```\n",
    "     \"Highly Satisfied\" → 0\n",
    "     \"Satisfied\" → 1\n",
    "     \"Neutral\" → 2\n",
    "     \"Not Satisfied\" → 3\n",
    "     \"Very Bad\" → 4\n",
    "     ```\n",
    "\n",
    "2. **Train-Test Split**:\n",
    "   - We perform an **80-20 split**, where **80%** of the data is used for training and **20%** for evaluation.\n",
    "   - **Stratified Sampling** ensures that the **class distribution remains proportional** in both training and test sets.\n",
    "\n",
    "## **Resulting Data Structure**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmthe\\AppData\\Local\\Temp\\ipykernel_9616\\1625214675.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[[\"Summary\", \"Text\"]] = data[[\"Summary\", \"Text\"]].fillna(\"\")\n",
      "C:\\Users\\mmthe\\AppData\\Local\\Temp\\ipykernel_9616\\1625214675.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"Combined_Text\"] = data[\"Summary\"] + \" \" + data[\"Text\"]\n",
      "C:\\Users\\mmthe\\AppData\\Local\\Temp\\ipykernel_9616\\1625214675.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"User_Satisfaction\"] = label_encoder.fit_transform(data[\"User_Satisfaction\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: (454763, 3), Test Set: (113691, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Select the required columns\n",
    "selected_columns = [\n",
    "    \"HelpfulnessNumerator\", \n",
    "    \"HelpfulnessDenominator\", \n",
    "    \"Summary\",  \n",
    "    \"Text\", \n",
    "    \"User_Satisfaction\"\n",
    "]\n",
    "data = data[selected_columns]\n",
    "# Fill missing values with an empty string\n",
    "data[[\"Summary\", \"Text\"]] = data[[\"Summary\", \"Text\"]].fillna(\"\")\n",
    "\n",
    "# Combine 'Summary' + 'Text' for text processing\n",
    "data[\"Combined_Text\"] = data[\"Summary\"] + \" \" + data[\"Text\"]\n",
    "\n",
    "# Encode 'User_Satisfaction' as numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "data[\"User_Satisfaction\"] = label_encoder.fit_transform(data[\"User_Satisfaction\"])\n",
    "\n",
    "# Train-Test Split (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[[\"HelpfulnessNumerator\", \"HelpfulnessDenominator\", \"Combined_Text\"]],\n",
    "    data[\"User_Satisfaction\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=data[\"User_Satisfaction\"]\n",
    ")\n",
    "\n",
    "# Display Data Splitting Results\n",
    "print(f\"Training Set: {X_train.shape}, Test Set: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataset consists of:\n",
    "- `HelpfulnessNumerator`\n",
    "- `HelpfulnessDenominator`\n",
    "- `Combined_Text`\n",
    "\n",
    "With target variable as `User_Satisfaction`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "### Multinomial Naive Bayes classifier\n",
    "For our baseline model we have chosen to use a Random Forest Classifier. This model was chosen as the base model for its ability to handle large datasets and It is less prone to overfitting because of its ability to combine multiple trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8156142526673176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.99      0.88     72624\n",
      "           2       0.90      0.44      0.59      8528\n",
      "           3       0.97      0.39      0.56      5954\n",
      "           4       0.92      0.44      0.59     16131\n",
      "           5       0.83      0.71      0.76     10454\n",
      "\n",
      "    accuracy                           0.82    113691\n",
      "   macro avg       0.88      0.59      0.68    113691\n",
      "weighted avg       0.83      0.82      0.79    113691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Convert text data into numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train['Combined_Text'])\n",
    "X_test_tfidf = vectorizer.transform(X_test['Combined_Text'])\n",
    "\n",
    "# Combine numerical features with text features\n",
    "X_train_combined = hstack([X_train_tfidf, X_train[['HelpfulnessNumerator', 'HelpfulnessDenominator']].values])\n",
    "X_test_combined = hstack([X_test_tfidf, X_test[['HelpfulnessNumerator', 'HelpfulnessDenominator']].values])\n",
    "\n",
    "# Train Random Forest Model\n",
    "rf_model = RandomForestClassifier(n_estimators=100,oob_score=True, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_combined, y_train)\n",
    "rf_preds = rf_model.predict(X_test_combined)\n",
    "\n",
    "# Evaluate Random Forest Model\n",
    "rf_acc = accuracy_score(y_test, rf_preds)\n",
    "rf_report = classification_report(y_test, rf_preds, target_names=[\"1\", \"2\", \"3\", \"4\", \"5\"])\n",
    "\n",
    "# Display Results\n",
    "print(\"Accuracy:\",rf_acc) \n",
    "print(rf_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model scores:\n",
    "- The baseline model has decent accuracy of 81.6%.\n",
    "- one major concern is that out of all of the classes 1 has the best overall performance when looking at the precision, recall and f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying convergence of the baseline model\n",
    "A random forest Classifier does not require a convergence check like logistic regression and neural networks. It does not have iterative optimization like gradient descent and the model stops growing once its predefined number of trees has been reached.\n",
    "\n",
    "One way to assess the \"convergence\" of the Random forest model is to monitor the out-of-bag (OOB) error. This is the prediction error calculated on the data points there were not used to train each indvidual tree, as the number of trees increases. When the OOB does not decrease significantly or begins to flatten out, this can serve as an indication that the model has converged\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Score: 0.8151058903208924\n"
     ]
    }
   ],
   "source": [
    "print(\"OOB Score:\", rf_model.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundational Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained transformer-based deep learning model developed by Google. It is designed to understand contextual meaning in text by processing words bidirectionally, meaning it considers both preceding and succeeding words to derive meaning. Unlike traditional NLP models that read text left to right or right to left, BERT captures the full context of a word in a sentence. This makes it highly effective for tasks like sentiment analysis, where word relationships significantly impact meaning. Instead of training a model from scratch, we can leverage transfer learning by fine-tuning BERT on our specific dataset, allowing it to adapt to sentiment-specific language patterns.\n",
    "\n",
    "BERT was pre-trained on large-scale text data from Wikipedia and BooksCorpus using two tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). In MLM, BERT learns to predict missing words in a sentence based on their surrounding context. In NSP, BERT learns to determine whether one sentence naturally follows another. These pre-training tasks give BERT a strong general understanding of language, making it highly transferable to sentiment classification tasks. Since our dataset consists of Amazon Fine Food Reviews, which include natural language reviews and corresponding sentiment labels, BERT’s pre-trained knowledge can be fine-tuned to classify reviews as positive, neutral, or negative. By applying transfer learning, BERT can adapt to domain-specific nuances in customer reviews while reducing training time and computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n",
      "True\n",
      "NVIDIA GeForce RTX 3060 Ti\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_scheduler, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#GPU Check\n",
    "print(torch.__version__)  \n",
    "print(torch.cuda.is_available())  \n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Load Pre-trained BERT Tokenizer and Model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_encoder.classes_))\n",
    "model.to(device)\n",
    "model.config.hidden_dropout_prob = 0.3\n",
    "model.config.attention_probs_dropout_prob = 0.3\n",
    "\n",
    "#Dataset class for BERT\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "#Put the train and test data into sets    \n",
    "train_dataset = ReviewDataset(X_train[\"Combined_Text\"].tolist(), y_train.tolist())\n",
    "test_dataset = ReviewDataset(X_test[\"Combined_Text\"].tolist(), y_test.tolist())\n",
    "\n",
    "#Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "#Loss Function/Optimizer\n",
    "class_weights = torch.tensor([0.5, 1.5, 1.8, 1.2, 1.3], dtype=torch.float).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": model.bert.encoder.layer[:-4].parameters(), \"lr\": 1e-6},  # Frozen layers (very low LR)\n",
    "    {\"params\": model.bert.encoder.layer[-4:].parameters(), \"lr\": 1e-5},  # Unfrozen layers (lower LR)\n",
    "    {\"params\": model.classifier.parameters(), \"lr\": 3e-5}  # Classifier (moderate LR)\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_training_steps = len(train_loader) * 3  # 3 epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=500, num_training_steps=num_training_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmthe\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequres_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#Training using the hugging face's trainer api\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results_frozen\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Keep only the best 2 models\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Disable logging to W&B\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     25\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     26\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)],  \u001b[38;5;66;03m# Stop training early if no improvement\u001b[39;00m\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#Train Call\u001b[39;00m\n",
      "File \u001b[1;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\mmthe\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\training_args.py:1791\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[1;32m-> 1791\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
      "File \u001b[1;32mc:\\Users\\mmthe\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\training_args.py:2313\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2309\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2310\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2311\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2312\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mmthe\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\utils\\generic.py:62\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     60\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32mc:\\Users\\mmthe\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\training_args.py:2186\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2187\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2188\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2189\u001b[0m         )\n\u001b[0;32m   2190\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[0;32m   2191\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def train_model(model, train_loader, optimizer, loss_fn, epochs=3, freeze=True, patience=2):\n",
    "    if freeze:\n",
    "        for param in model.bert.parameters():\n",
    "            param.requires_grad = False  # Freeze BERT layers\n",
    "\n",
    "    model.train()\n",
    "    best_loss = float(\"inf\")  # Track best loss\n",
    "    patience_counter = 0  # Track early stopping\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in loop:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids, attention_mask, labels = (\n",
    "                batch[\"input_ids\"].to(device),\n",
    "                batch[\"attention_mask\"].to(device),\n",
    "                batch[\"labels\"].to(device),\n",
    "            )\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loop.set_description(f\"Epoch {epoch+1}\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # 🔥 Early Stopping Condition\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0  # Reset patience if loss improves\n",
    "        else:\n",
    "            patience_counter += 1  # Increase patience counter\n",
    "\n",
    "        if patience_counter >= patience:  # Stop training if no improvement for 'patience' epochs\n",
    "            print(\"Early stopping triggered. Training stopped.\")\n",
    "            break  # Stop training early\n",
    "\n",
    "train_model(model, train_loader, optimizer, loss_fn, epochs=5, freeze=True, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids, attention_mask, labels = (\n",
    "                batch[\"input_ids\"].to(device),\n",
    "                batch[\"attention_mask\"].to(device),\n",
    "                batch[\"labels\"].to(device),\n",
    "            )\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(true_labels, preds)\n",
    "    print(f\"Model Accuracy: {acc:.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(true_labels, preds, target_names=label_encoder.classes_))\n",
    "\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine Tuning\n",
    "for param in model.bert.encoder.layer[-2:].parameters():\n",
    "    param.requires_grad = True  # Unfreeze last two layers\n",
    "\n",
    "#Optimzier\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": model.bert.encoder.layer[:-4].parameters(), \"lr\": 1e-6},  # Frozen layers (very low LR)\n",
    "    {\"params\": model.bert.encoder.layer[-4:].parameters(), \"lr\": 1e-5},  # Unfrozen layers (lower LR)\n",
    "    {\"params\": model.classifier.parameters(), \"lr\": 3e-5}  # Classifier (higher LR)\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, weight_decay=0.01)\n",
    "num_training_steps = len(train_loader) * 5  # 5 epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=500, num_training_steps=num_training_steps)\n",
    "\n",
    "\n",
    "train_model(model, train_loader, optimizer, loss_fn, epochs=5, freeze=False)\n",
    "\n",
    "\n",
    "evaluate_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
