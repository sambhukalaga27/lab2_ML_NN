{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as a dll could not be loaded.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresDllLoad'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorFlow/Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense\n",
    "\n",
    "# PyTorch & Hugging Face imports\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow recognizes GPU: False\n",
      "PyTorch recognizes GPU: True\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow recognizes GPU:\", len(tf.config.list_physical_devices('GPU')) > 0)\n",
    "print(\"PyTorch recognizes GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000  # maximum number of words to consider\n",
    "max_len = 200      # maximum review length (in tokens)\n",
    "\n",
    "# Fit the tokenizer on training data\n",
    "tokenizer_keras = Tokenizer(num_words=max_words)\n",
    "tokenizer_keras.fit_on_texts(dataset[\"train\"][\"text\"])\n",
    "\n",
    "# Tokenize and pad training and test texts\n",
    "X_train = tokenizer_keras.texts_to_sequences(dataset[\"train\"][\"text\"])\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "y_train = np.array(dataset[\"train\"][\"label\"])\n",
    "\n",
    "X_test = tokenizer_keras.texts_to_sequences(dataset[\"test\"][\"text\"])\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "y_test = np.array(dataset[\"test\"][\"label\"])\n",
    "\n",
    "# Build the baseline model\n",
    "baseline_model = Sequential([\n",
    "    Embedding(max_words, 128, input_length=max_len),\n",
    "    Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=4),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "baseline_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "baseline_model.summary()\n",
    "\n",
    "# Optional: Place training inside a GPU context if you want to be explicit.\n",
    "# If TensorFlow is GPU-enabled, it should automatically run on GPU without this.\n",
    "with tf.device('/GPU:0'):\n",
    "    history_baseline = baseline_model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=5,\n",
    "        validation_data=(X_test, y_test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define a tokenization function for the dataset\n",
    "def tokenize_function(examples):\n",
    "    return bert_tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Tokenize the datasets (batched processing)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "# Remove the original text column to speed up training\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "# Set the format to PyTorch tensors\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "# Load pre-trained BERT for sequence classification (binary classification: num_labels=2)\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_results\",\n",
    "    num_train_epochs=2,               # adjust epochs as needed\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./bert_logs',\n",
    "    # Set this to False to enable GPU if available\n",
    "    no_cuda=False\n",
    ")\n",
    "\n",
    "# Define the Trainer for BERT\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the BERT model (bottleneck fine-tuning)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert_model.bert.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Continue fine-tuning with a few more epochs\n",
    "training_args.num_train_epochs = 2  # Additional epochs\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_baseline.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_baseline.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Baseline Model Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
